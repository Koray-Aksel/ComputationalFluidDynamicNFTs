{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ef80aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin ScrapeTweets\n"
     ]
    }
   ],
   "source": [
    "## ScrapeTweets.py by Ryan Farber 2022-04-16 (@TheLunaLabs, @ToTheMoonsNFT)\n",
    "\"\"\"\n",
    "I decided to switch from single scripts to an object oriented approach.\n",
    "\n",
    "NOTE: need to loop over tokens to actually get all values!!!\n",
    "\n",
    "NOTE: need to see if there's alt text for this meme saying roo\n",
    "      https://twitter.com/metacon68/status/1515389758842626048\n",
    "      (leave this for later)\n",
    "\n",
    "NOTE: for tweets not by RooTroopNFT I should check if content says \"roo\"\n",
    "      in it.\n",
    "\n",
    "Priorities:\n",
    "  1) loop over all tokens!\n",
    "  2) if not by RooTroopNFT, skip likes + RTs\n",
    "  3) outside of tweets in the troop-raids channel, blindly search for \"roo\"\n",
    "\"\"\"\n",
    "import os\n",
    "import re\n",
    "import ast\n",
    "import glob\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import asyncio\n",
    "print(\"Begin ScrapeTweets\")\n",
    "\n",
    "S_PER_MINUTE = 60\n",
    "S_PER_HOUR   = S_PER_MINUTE * 60\n",
    "S_PER_DAY    = S_PER_HOUR * 24\n",
    "S_PER_MONTH  = S_PER_DAY * 31\n",
    "S_PER_YEAR   = S_PER_MONTH * 12\n",
    "\n",
    "start = time.time()\n",
    "class ScrapeTweets(object):\n",
    "  def __init__(self):\n",
    "    '''\n",
    "    just initializes some strings to reduce duplication / for convenience\n",
    "    '''\n",
    "    self.twitter_api_base = \"https://api.twitter.com/2/tweets/\"\n",
    "    self.curl_base = \"curl --request GET --url '\"\n",
    "    self.curl_header = \"' --header 'Authorization: Bearer \"\n",
    "    self.data_dir = \"twitter_data\"\n",
    "    self.dtypes = [\"Likes\", \"Retweets\", \"QuoteTweets\", \"Replies\"]\n",
    "    self.include_text = '\"includes\":\\{\"users\":\\[\\{'\n",
    "    self.meta_text = '\"meta\":\\{\"'\n",
    "    self.result_text = '\"result_count\":'\n",
    "    self.created_text = '\"created_at\":\"'\n",
    "\n",
    "    self.fname_activity = self.data_dir + \"/activity_by_user.json\"\n",
    "    self.fname_user_info = self.data_dir + \"/user_info.json\"\n",
    "\n",
    "    self.special_tweeters = {\"1447280926967304195\":\"rootroopnft\", \n",
    "                             \"1477912158730170370\":\"troopsales\"}\n",
    "    self.max_loops = 300 # limit == 30,000 likes, RTs (note, max 900 requests per 15 minutes\n",
    "    self.continuously_scrape_sleep_time = 10 # seconds\n",
    "\n",
    "    self.keyword_query  = \"(Rooty Roo OR Rooty Woo OR rootywoo OR Roo Troop OR rootroop\"\n",
    "    self.keyword_query += \" OR rootroops OR tree roo OR roo bounty OR roo bounties\"\n",
    "    self.keyword_query += \" OR rootyroo OR RootyRoo OR rootroopnft OR troopsales)\"\n",
    "\n",
    "    ## 900 requests per 15 minutes max but likes/retweets lookup is 75 (total or each so total is 150? unclear)\n",
    "    self.api_calls_struct = {\"time_limit_s\": 15*S_PER_MINUTE, \n",
    "                             \"max_calls_per_time_limit_all\": 900,\n",
    "                             \"max_calls_per_time_limit\": {\n",
    "                                                      \"Tweets\": 900,\n",
    "                                                      \"keyword\": 180,\n",
    "                                                      \"Likes\": 75,\n",
    "                                                      \"Retweets\": 75,\n",
    "                                                      \"QuoteTweets\": 75,\n",
    "                                                      \"Replies\": 180,\n",
    "                                                      \"Users\": 900},\n",
    "                             \"buffer_size\": 7,\n",
    "                             \"call_times\":{\n",
    "                                           \"Tweets\": [],\n",
    "                                           \"keyword\": [],\n",
    "                                           \"Likes\": [],\n",
    "                                           \"Retweets\": [],\n",
    "                                           \"QuoteTweets\": [],\n",
    "                                           \"Replies\": [],\n",
    "                                           \"Users\": []\n",
    "                                          },\n",
    "                             \"call_count\":0,\n",
    "                             \"fname\":self.data_dir + \"/api_call_times.txt\",\n",
    "                             \"fname_stats\":self.data_dir + \"/api_call_stats.txt\"\n",
    "                            }\n",
    "\n",
    "    os.system(\"mkdir -p \" + self.data_dir)\n",
    "    if not os.path.exists(self.api_calls_struct[\"fname\"]) or \\\n",
    "           os.stat(self.api_calls_struct[\"fname\"]).st_size == 0:\n",
    "      with open(self.api_calls_struct[\"fname\"], \"w\") as fid:\n",
    "        fid.write(str(self.api_calls_struct[\"call_times\"]))\n",
    "      # end with\n",
    "    # end if\n",
    "\n",
    "    if not os.path.exists(self.api_calls_struct[\"fname_stats\"]) or \\\n",
    "       os.stat(self.api_calls_struct[\"fname_stats\"]).st_size == 0:\n",
    "      with open(self.api_calls_struct[\"fname_stats\"], \"w\") as fid:\n",
    "        api_call_stats = {\n",
    "          \"1H\": \n",
    "           {\n",
    "            \"time_in_s\": 3600.0,\n",
    "            \"call_times_this\": [], \n",
    "            \"call_times_past_sum\": 0.0, \n",
    "            \"call_times_past_num\": 0.0,\n",
    "            \"call_times_past_len\": []}, \n",
    "          \"1D\": \n",
    "           {\n",
    "            \"time_in_s\": 3600.0*24.0,\n",
    "            \"call_times_this\": [], \n",
    "            \"call_times_past_sum\": 0.0, \n",
    "            \"call_times_past_num\": 0.0,\n",
    "            \"call_times_past_len\": []}, \n",
    "          \"30D\": \n",
    "           {\n",
    "            \"time_in_s\": 3600.0*24.0*30.0,\n",
    "            \"call_times_this\": [], \n",
    "            \"call_times_past_sum\": 0.0, \n",
    "            \"call_times_past_num\": 0.0,\n",
    "            \"call_times_past_len\": []}\n",
    "        }\n",
    "        fid.write(str(api_call_stats))\n",
    "      # end with open\n",
    "    # end if\n",
    "\n",
    "    self.user_dict = self.safe_load(self.fname_user_info)\n",
    "    if self.user_dict == {}:\n",
    "      self.user_dict[\"userId_to_username\"] = {}\n",
    "      self.user_dict[\"username_to_userId\"] = {}\n",
    "    # end if\n",
    "\n",
    "    with open(\"discord_data/linked_3.json\", \"r\") as fid:\n",
    "      line = ast.literal_eval(fid.read())\n",
    "      self.linked_usernames = []\n",
    "      self.linked_userIds = []\n",
    "      for el in line:\n",
    "        self.linked_usernames.append(str(el[\"handle\"]))\n",
    "        self.linked_userIds.append(str(el[\"id\"]))\n",
    "      \n",
    "        self.user_dict[\"userId_to_username\"][str(el[\"id\"])] = str(el[\"handle\"])\n",
    "        self.user_dict[\"username_to_userId\"][str(el[\"handle\"])] = str(el[\"id\"])\n",
    "      # end for\n",
    "    # end with open\n",
    "    self.safe_save(self.fname_user_info, self.user_dict)\n",
    "    print(\"loaded linked_usernames!\")    \n",
    "\n",
    "    self.activity_by_user = self.safe_load(self.fname_activity)\n",
    "  # end __init__\n",
    "\n",
    "  #=====================================================\n",
    "  #=====================================================\n",
    "  #=====================================================\n",
    "\n",
    "  # my laptop is almost 10 years old so I assume there's some\n",
    "  # malware on here now (although I still haven't lost any crypto/nfts\n",
    "  # in the 5 years I've been around...), so I make it harder for bots\n",
    "  #  to snoop ;)\n",
    "  def init_auth(self):\n",
    "    '''\n",
    "    gets twitter authentication token from local file and saves to self.auth\n",
    "      inputs:  none\n",
    "      outputs: none\n",
    "      side effects: self.auth is assigned a value.\n",
    "    '''\n",
    "    print(\"begin init_auth\")\n",
    "    auth_str = \"\"\n",
    "    with open(\"git_ignores_me.mp4\", \"r\") as fid:\n",
    "      for line in fid:\n",
    "        cur_str = line.split(\" = \")[1]\n",
    "        cur_str = cur_str[1:-2] # remove quotes and newline char\n",
    "\n",
    "        auth_str += cur_str\n",
    "      # end for line\n",
    "    # end with open\n",
    "    self.auth = auth_str\n",
    "\n",
    "    print(\"success init_auth\")\n",
    "  # end init_auth\n",
    "\n",
    "  #=====================================================\n",
    "  #=====================================================\n",
    "  #=====================================================\n",
    "\n",
    "  def init_tweet(self, tweet_url):\n",
    "    print(\"begin init_tweet\")\n",
    "\n",
    "    tweet_id = tweet_url.split(\"status/\")[1]\n",
    "    if \"?\" in tweet_id:\n",
    "      tweet_id = tweet_id.split(\"?\")[0]\n",
    "    # end if\n",
    "\n",
    "    creator_username = tweet_url.split(\"/status\")[0\n",
    "                                    ].split(\"twitter.com/\")[1]\n",
    "\n",
    "    print(\"success init_tweet\")\n",
    "    return [tweet_id, creator_username]\n",
    "  # end init_tweet\n",
    "\n",
    "  #=====================================================\n",
    "  #=====================================================\n",
    "  #=====================================================\n",
    "\n",
    "  def get_tweet_time_s(self, tweet_time):\n",
    "    #print(\"begin get_tweet_time_s\")\n",
    "\n",
    "    yy,mo,dd = tweet_time.split(\"-\")\n",
    "    dd,hh         = dd.split(\"T\")\n",
    "    hh,mi,ss = hh.split(\":\")\n",
    "    ss = ss[:-1]\n",
    "    tweet_time_s = float(yy)*S_PER_YEAR   + float(mo)*S_PER_MONTH + \\\n",
    "                        float(dd)*S_PER_DAY    + float(hh)*S_PER_HOUR  + \\\n",
    "                        float(mi)*S_PER_MINUTE + float(ss)\n",
    "    return tweet_time_s\n",
    "\n",
    "    print(\"success get_tweet_time_s\")\n",
    "  # end get_tweet_time_s\n",
    "\n",
    "  #=====================================================\n",
    "  #=====================================================\n",
    "  #=====================================================\n",
    "\n",
    "  async def save_url_to_file(self, url, fname):\n",
    "    print(\"begin save_url_to_file\")\n",
    "    await asyncio.sleep(0.02)\n",
    "\n",
    "    self.api_calls_struct[\"call_times\"] = self.safe_load(self.api_calls_struct[\"fname\"])\n",
    "    api_call_stats = self.safe_load(self.api_calls_struct[\"fname_stats\"])\n",
    "\n",
    "    if \"query=conversation_id\" in url:\n",
    "      dtype = \"Replies\"\n",
    "    elif \"query\" in url:\n",
    "      dtype = \"keyword\"\n",
    "    elif \"/liking_users?\" in url:\n",
    "      dtype = \"Likes\"\n",
    "    elif \"/retweeted_by?\" in url:\n",
    "      dtype = \"Retweets\"\n",
    "    elif \"/quote_tweets?&expansions=author_id&\" in url:\n",
    "      dtype = \"QuoteTweets\"\n",
    "    elif \"https://api.twitter.com/2/tweets?ids=\" in url:\n",
    "      dtype = \"Tweets\"\n",
    "    elif \"https://api.twitter.com/2/users/\" in url:\n",
    "      dtype = \"Users\"\n",
    "    else:\n",
    "      print(\"uhhhh url not recognized in api_calls_struct\")\n",
    "      print(\"url: \", url)\n",
    "      raise\n",
    "    # end if/elifs\n",
    "    print(\"dtype: \", dtype)\n",
    "\n",
    "    call_times = self.api_calls_struct[\"call_times\"][dtype]\n",
    "    total_calls = 0\n",
    "    for key in self.api_calls_struct[\"call_times\"].keys():\n",
    "      total_calls += len(self.api_calls_struct[\"call_times\"][key])\n",
    "    # end for\n",
    "    max_calls_per_time_limit = self.api_calls_struct[\"max_calls_per_time_limit\"][dtype]\n",
    "\n",
    "    while (len(call_times) > (max_calls_per_time_limit - \\\n",
    "          self.api_calls_struct[\"buffer_size\"])) or \\\n",
    "          (total_calls > (self.api_calls_struct[\"max_calls_per_time_limit_all\"] \\\n",
    "        - self.api_calls_struct[\"buffer_size\"])):\n",
    "\n",
    "      print(\"about to sleep a minute because too many recent requests!\")\n",
    "      await asyncio.sleep(60.1)\n",
    "      for key in self.api_calls_struct[\"call_times\"].keys():\n",
    "        call_times_loop_arr = self.api_calls_struct[\"call_times\"][key]\n",
    "        offset = 0\n",
    "        for ii in range(len(call_times_loop_arr)):\n",
    "          if time.time() - self.api_calls_struct[\"call_times\"][key][ii+offset] > 15*S_PER_MINUTE:\n",
    "            del self.api_calls_struct[\"call_times\"][key][ii+offset]\n",
    "            offset -= 1\n",
    "          # end if\n",
    "        # end for\n",
    "        await asyncio.sleep(0.03)\n",
    "      # end for\n",
    "    # end while\n",
    "\n",
    "    flag = True\n",
    "    cnt = 0\n",
    "    while flag:\n",
    "      result = os.system(self.curl_base + url + self.curl_header + self.auth + \n",
    "                \"' >> \" + fname)\n",
    "      \n",
    "      with open(fname, \"r\") as fid:\n",
    "        line = fid.read()\n",
    "        if '\"status\":503' not in line and '\"status\":443' not in line \\\n",
    "          and result == 0:\n",
    "          flag = False\n",
    "          break\n",
    "        # end if\n",
    "      # end with\n",
    "      print(\"curl failed, sleeping 60s then trying again. will try at most 10 times\")\n",
    "      print(\"current cnt (to be incremented after sleep: \", cnt)\n",
    "      await asyncio.sleep(60.1)\n",
    "      os.system(\"rm \" + fname)\n",
    "      cnt += 1\n",
    "      if cnt > 10:\n",
    "        print(\"curl failed 10 times! crashing now\")\n",
    "        raise\n",
    "      # end if\n",
    "    # end while\n",
    "    await asyncio.sleep(0.1)\n",
    "    self.api_calls_struct[\"call_count\"] += 1\n",
    "    \n",
    "    tnow = time.time()\n",
    "    self.api_calls_struct[\"call_times\"][dtype].append(tnow)\n",
    "\n",
    "    for key in api_call_stats.keys():\n",
    "      ## check if it's a new time unit compared to old ones...\n",
    "      ## if so, we compute the avg, update sum and num flush call_times_this\n",
    "      ## and then whether it's a new time unit or not we append tnow :)\n",
    "\n",
    "      if len(api_call_stats[key][\"call_times_this\"]) > 0:\n",
    "        \n",
    "        t0 = api_call_stats[key][\"call_times_this\"][ 0]\n",
    "        tf = api_call_stats[key][\"call_times_this\"][-1]\n",
    "        if (tf - t0) > api_call_stats[key][\"time_in_s\"]:\n",
    "          num_calls = len(api_call_stats[key][\"call_times_this\"])\n",
    "\n",
    "          if len(api_call_stats[key][\"call_times_past_len\"])+1 > int(1e6)-1:\n",
    "            api_call_stats[key][\"call_times_past_len\"] = []\n",
    "          # end if\n",
    "\n",
    "          api_call_stats[key][\"call_times_past_len\"].append(num_calls)\n",
    "          api_call_stats[key][\"call_times_past_sum\"] += num_calls\n",
    "          api_call_stats[key][\"call_times_past_num\"] += 1\n",
    "        # end if\n",
    "      # end if\n",
    "      api_call_stats[key][\"call_times_this\"].append(tnow)\n",
    "    # end for\n",
    "\n",
    "    self.safe_save(self.api_calls_struct[\"fname\"], self.api_calls_struct[\"call_times\"])\n",
    "    self.safe_save(self.api_calls_struct[\"fname_stats\"], api_call_stats)\n",
    "\n",
    "    print(\"success save_url_to_file\")\n",
    "  # end save_url_to_file\n",
    "\n",
    "  #=====================================================\n",
    "  #=====================================================\n",
    "  #=====================================================\n",
    "\n",
    "  def build_activity_from_pairs(self, pairs, activity, line):\n",
    "    for key in pairs.keys():\n",
    "      activity += pairs[key]\n",
    "\n",
    "      if key in line:\n",
    "        vals = line.split(key)[1:]\n",
    "      else:\n",
    "        vals = []\n",
    "        activity += 2*\" \"\n",
    "      # end if/else\n",
    "\n",
    "      for val in vals:\n",
    "        val = val.split('\"')[0]\n",
    "        activity += '\"' + val + '\", '\n",
    "      # end for vals\n",
    "      activity = activity[:-2] + '], '\n",
    "    # end for\n",
    "    activity = activity[:-2]\n",
    "    return activity\n",
    "  # end build_activity_from_pairs\n",
    "\n",
    "  #=====================================================\n",
    "  #=====================================================\n",
    "  #=====================================================\n",
    "\n",
    "  def update_user_dict(self, line):\n",
    "    user_ids = []\n",
    "    usernames = []\n",
    "    keys = ['\"id\":\"', '\"username\":\"']\n",
    "    for ii, key in enumerate(keys):\n",
    "      if key in line:\n",
    "        vals = line.split(key)[1:]\n",
    "      else:\n",
    "        vals = []\n",
    "      # end if/else\n",
    "\n",
    "      for val in vals:\n",
    "        val = val.split('\"')[0]\n",
    "        if ii == 0:\n",
    "          user_ids.append(val)\n",
    "        else:\n",
    "          usernames.append(val)\n",
    "        # end if/else\n",
    "      # end for\n",
    "    # end for\n",
    "    for ii in range(len(usernames)):\n",
    "      self.user_dict[\"userId_to_username\"][ user_ids[ii]] = usernames[ii]\n",
    "      self.user_dict[\"username_to_userId\"][usernames[ii]] = user_ids[ ii]\n",
    "    # end for\n",
    "\n",
    "    self.safe_save(self.fname_user_info, self.user_dict)\n",
    "  # end update_user_dict\n",
    "\n",
    "  #=====================================================\n",
    "  #=====================================================\n",
    "  #=====================================================\n",
    "\n",
    "  async def fetch_data(self, tweet_url, dtype):\n",
    "    '''\n",
    "    for a given tweet, fetch the users that liked, retweeted, quote-tweeted,\n",
    "    or replied, and saves json response to a file for later processing.\n",
    "      inputs: \n",
    "        tweet_url (type == str)\n",
    "        dtype (type == str) \n",
    "          valid values == 'Likes', 'Retweets', 'QuoteTweets', 'Replies',\n",
    "      outputs: none\n",
    "      side effects: writes files with the responses\n",
    "    '''\n",
    "    print(\"begin fetch_data\")\n",
    "\n",
    "    self_tweet_id, self_creator_username = self.init_tweet(tweet_url)\n",
    "\n",
    "    fname = self.data_dir + \"/\" + dtype + \"_\" + self_creator_username + \"_\" + \\\n",
    "               self_tweet_id + \".txt\"\n",
    "    print(\"fname: \", fname)\n",
    "\n",
    "    url = self.twitter_api_base[:-1] + \"?ids=\" + self_tweet_id + \\\n",
    "          \"&tweet.fields=public_metrics\"\n",
    "\n",
    "    await self.save_url_to_file(url, fname)\n",
    "\n",
    "    url_og = self.twitter_api_base\n",
    "    if dtype != \"Replies\":\n",
    "      url_og += self_tweet_id\n",
    "    # end if\n",
    "\n",
    "    if   dtype == \"Likes\":\n",
    "      url_og += \"/liking_users?\"\n",
    "    elif dtype == \"Retweets\":\n",
    "      url_og += \"/retweeted_by?\"\n",
    "    elif dtype == \"QuoteTweets\":\n",
    "      url_og += \"/quote_tweets?&expansions=author_id&\"\n",
    "    elif dtype == \"Replies\":\n",
    "      url_og += \"search/recent?query=conversation_id:\" + \\\n",
    "        self_tweet_id + \"&expansions=author_id,in_reply_to_user_id&\"\n",
    "    else:\n",
    "      print(\"error! expected dtype in 'likes', 'retweets' but received: \", \n",
    "            dtype)\n",
    "      raise\n",
    "    # end if/elif\n",
    "    url_og += \"user.fields=username&max_results=100&tweet.fields=public_metrics\"\n",
    "    url = url_og + \"\"\n",
    "\n",
    "    token = \"\"\n",
    "    loop  = True\n",
    "    num_loops = 0\n",
    "    while loop and num_loops < self.max_loops:\n",
    "      await asyncio.sleep(0.1) # otherwise Cntrl-C alwyas just kills the curl :(\n",
    "      print(\"num_loops: \", num_loops)\n",
    "      num_loops += 1\n",
    "\n",
    "      await self.save_url_to_file(url, fname)\n",
    "\n",
    "      if os.stat(fname).st_size == 0:\n",
    "        print(\"error, didn't grab any data, probably url has a bug\")\n",
    "        raise\n",
    "      # end if\n",
    "\n",
    "      ## check if we grabbed all of them or not\n",
    "      with open(fname, \"r\") as fid:\n",
    "        for line in fid:\n",
    "          #print(\"line1: \", line)\n",
    "          inds = [m.start() for m in re.finditer(self.meta_text, line)]\n",
    "          line = line[inds[-1]:]\n",
    "          print(\"line2: \", line)\n",
    "          print(\"inds: \", inds)\n",
    "\n",
    "          substr = '\"next_token\":\"'\n",
    "          if substr in line:\n",
    "            ind   = line.find(substr) + len(substr)\n",
    "            line  = line[ind:]\n",
    "            token = line.split('\"')[0]\n",
    "            print(\"token: \", token)\n",
    "\n",
    "            url = url_og + \"&pagination_token=\" + token\n",
    "          else:\n",
    "            print(\"substr not in line!\")\n",
    "            print(\"substr: \", substr)\n",
    "            loop = False\n",
    "          # end if\n",
    "        # end for\n",
    "      # end with\n",
    "    # end while\n",
    "\n",
    "    print(\"success fetch_data\")\n",
    "  # end fetch_data\n",
    "\n",
    "  #=====================================================\n",
    "  #=====================================================\n",
    "  #=====================================================\n",
    "\n",
    "  async def fetch_activity(self, tweet_url, creation_time, update=False):\n",
    "    '''\n",
    "    for a given tweet, fetch the users that liked, retweeted, quote-tweeted,\n",
    "    or replied and saves json response to a single file for later processing.\n",
    "      inputs: tweet_url (type == str), (optional) update (type == boolean)\n",
    "      outputs: none\n",
    "      side effects: calls fetch_likes, fetch_retweets, fetch_quote_tweets,\n",
    "        fetch_replies and generates a dictionary containing all that data\n",
    "        and saves to a file. Assigns self.activity\n",
    "    '''\n",
    "    print(\"begin fetch_activity\")\n",
    "\n",
    "    self_tweet_id, self_creator_username = self.init_tweet(tweet_url)\n",
    "\n",
    "    fname_out = self.data_dir + \"/activity_\" + self_creator_username + \"_\" + \\\n",
    "                   self_tweet_id + \".txt\"\n",
    "\n",
    "    if os.path.isfile(fname_out) and update == False and os.stat(fname_out).st_size != 0:\n",
    "      with open(fname_out, \"r\") as fid:\n",
    "        for line in fid:\n",
    "          activity = line\n",
    "          self.activity = ast.literal_eval(activity)\n",
    "          print(\"successfully loaded existing activity\")\n",
    "          return\n",
    "        # end for\n",
    "      # end with open\n",
    "    # end if\n",
    "\n",
    "    for dtype in self.dtypes:\n",
    "      if len(glob.glob(self.data_dir + \"/\" + dtype + \"*\" + self_tweet_id + \".txt\")) == 0:\n",
    "        await self.fetch_data(tweet_url, dtype)\n",
    "      # end if\n",
    "    # end for\n",
    "\n",
    "    activity  = '{\"' + self_tweet_id + '\":{\"tweet_url\":\"' + tweet_url + '\", '\n",
    "    activity += '\"tweet_author_username\":\"' + self_creator_username   + '\", '\n",
    "    activity += '\"tweet_created_at\":\"' + creation_time + '\", '\n",
    "\n",
    "    dtype = \"Likes\"\n",
    "    fname = self.data_dir + \"/\" + dtype + \"_\" + self_creator_username \\\n",
    "               + \"_\" + self_tweet_id + \".txt\"\n",
    "\n",
    "    with open(fname, \"r\") as fid:\n",
    "      for line in fid:\n",
    "        line = line.split('\"public_metrics\":{')[1].split(\"}\")[0]\n",
    "        print(\"line: \", line)\n",
    "      # end for line\n",
    "    # end with open\n",
    "    activity += line + \"}}\"\n",
    "    activity = ast.literal_eval(activity)\n",
    "    print(\"activity: \", activity)\n",
    "\n",
    "    for dtype in self.dtypes:\n",
    "      print(\"dtype: \", dtype)\n",
    "      fname = self.data_dir + \"/\" + dtype + \"_\" + self_creator_username \\\n",
    "              + \"_\" + self_tweet_id + \".txt\"\n",
    "\n",
    "      activity[self_tweet_id][dtype] = {\"ids\":[], \"usernames\":[]}\n",
    "      if dtype in [\"Replies\",\"QuoteTweets\"]:\n",
    "        activity[self_tweet_id][dtype][\"contents\" ] = []\n",
    "        activity[self_tweet_id][dtype][\"tweet_ids\"] = []\n",
    "      with open(fname, \"r\") as fid:\n",
    "        line = fid.read()\n",
    "      # end with\n",
    "      lines = line.split(\"}{\")[1:]\n",
    "      for line in lines:\n",
    "        line = \"{\" + line\n",
    "        if \"next_token\" in line:\n",
    "          print(\"hi\")\n",
    "          line = line + \"}\"\n",
    "        # end if\n",
    "        vals = ast.literal_eval(line)\n",
    "        print(\"vals.keys: \", vals.keys())\n",
    "        if \"data\" not in vals.keys():\n",
    "          continue\n",
    "        # end if\n",
    "        if \"includes\" not in vals.keys():\n",
    "          for val in vals[\"data\"]:\n",
    "            print(\"val: \", val)\n",
    "            user_id = val[\"id\"]\n",
    "            username = val[\"username\"]\n",
    "            self.user_dict[\"userId_to_username\"][user_id]  = username\n",
    "            self.user_dict[\"username_to_userId\"][username] = user_id\n",
    "\n",
    "            activity[self_tweet_id][dtype][\"ids\"].append(user_id)\n",
    "            activity[self_tweet_id][dtype][\"usernames\"].append(username)\n",
    "          # end for\n",
    "        else:\n",
    "          for val in vals[\"includes\"][\"users\"]:\n",
    "            print(\"includes val users: \", val)\n",
    "            user_id = val[\"id\"]\n",
    "            username = val[\"username\"]\n",
    "            self.user_dict[\"userId_to_username\"][user_id]  = username\n",
    "            self.user_dict[\"username_to_userId\"][username] = user_id\n",
    "          # end for\n",
    "          for val in vals[\"data\"]:\n",
    "            print(\"includes val data: \", val)\n",
    "            tweet_id = val[\"id\"]\n",
    "            author_id = val[\"author_id\"]\n",
    "            text = val[\"text\"]\n",
    "            \n",
    "            activity[self_tweet_id][dtype][\"ids\"].append(author_id)\n",
    "            activity[self_tweet_id][dtype][\"contents\"].append(text)\n",
    "            activity[self_tweet_id][dtype][\"usernames\"].append(self.user_dict[\"userId_to_username\"][author_id])\n",
    "            activity[self_tweet_id][dtype][\"tweet_ids\"].append(tweet_id)\n",
    "          # end for\n",
    "        # end if\n",
    "      # end for lines\n",
    "    # end for dtypes\n",
    "\n",
    "    ## safe_save here!!\n",
    "    self.safe_save(self.fname_user_info, self.user_dict)\n",
    "\n",
    "    with open(fname_out, \"w\") as fid:\n",
    "      fid.write(str(activity))\n",
    "    # end with\n",
    "\n",
    "    self.activity = activity\n",
    "\n",
    "    # if all that was successful, we delete the old files to keep\n",
    "    # things tidy :)\n",
    "    for dtype in self.dtypes:\n",
    "      fname = self.data_dir + \"/\" + dtype + \"_\" + self_creator_username \\\n",
    "              + \"_\" + self_tweet_id + \".txt\"\n",
    "      os.system(\"rm \" + fname)\n",
    "    # end for\n",
    "\n",
    "    print(\"success fetch_activity\")\n",
    "  # end fetch_activity\n",
    "\n",
    "  #=====================================================\n",
    "  #=====================================================\n",
    "  #=====================================================\n",
    "\n",
    "  async def process_all_tweets_by_user(self, user_id, update = False):\n",
    "    print(\"begin process_all_tweets_by_user\")\n",
    "\n",
    "    try:\n",
    "      username = self.special_tweeters[user_id]\n",
    "    except:\n",
    "      print(\"error: user_id not in special tweeters!\")\n",
    "      raise\n",
    "    # end try/except\n",
    "\n",
    "    fname = self.data_dir + \"/TweetsByUser_\" + user_id + \".txt\"\n",
    "\n",
    "    await asyncio.sleep(0.1)\n",
    "    if not os.path.isfile(fname) or update == True or os.stat(fname).st_size == 0:\n",
    "      await self.fetch_all_tweets_by_user(user_id)\n",
    "    # end if\n",
    "    await asyncio.sleep(0.1)\n",
    "\n",
    "    line = self.safe_load(fname)\n",
    "    if \"tweets_processed\" in line[-1].keys():\n",
    "      processed_tweets = line[-1][\"tweets_processed\"]\n",
    "    else:\n",
    "      line[-1][\"tweets_processed\"] = []\n",
    "      processed_tweets = []\n",
    "    # end if/else\n",
    "\n",
    "    for ii in range(len(line)):\n",
    "      await asyncio.sleep(0.1)\n",
    "      for jj in range(len(line[ii][\"data\"])):\n",
    "        arr = line[ii][\"data\"][jj]\n",
    "        tweet_id = arr[\"id\"]\n",
    "        creation_time = arr[\"created_at\"]\n",
    "        text = arr[\"text\"]\n",
    "\n",
    "        creation_time_s = self.get_tweet_time_s(creation_time)\n",
    "        if creation_time_s > line[-1][\"last_tweet_time_s\"]:\n",
    "          line[-1][\"last_tweet_time_s\"] = creation_time_s\n",
    "        # end if\n",
    "\n",
    "        ## check if we are in the refresh time window or not\n",
    "        num_days = 1\n",
    "        tnow = datetime.datetime.now() - datetime.timedelta(days=num_days)\n",
    "        tnow = tnow.strftime(\"%Y-%m-%dT%H:%M:%S.000Z\")\n",
    "        tnow = self.get_tweet_time_s(tnow)\n",
    "        refresh_time = tnow\n",
    "\n",
    "        if tweet_id in processed_tweets and creation_time_s < refresh_time:\n",
    "          print(\"tweet_id in processed_tweets and older than \" + str(num_days) + \" days, so skipping\")\n",
    "          continue\n",
    "        # end if\n",
    "        print(\"tweet_id not in processed_tweets\")\n",
    "\n",
    "        RT_key = \"RT @\"\n",
    "        if RT_key == text[:len(RT_key)]:\n",
    "          print(\"it's a RT so we skip\")\n",
    "          continue\n",
    "        # end if\n",
    "        print(\"not a retweet so continuing\")\n",
    "        #input(\">>\")\n",
    "\n",
    "        tweet_url = \"https://twitter.com/\" + username + \"/status/\" + tweet_id\n",
    "        await self.fetch_activity(tweet_url, creation_time)\n",
    "        self.process_url_activity()\n",
    "\n",
    "        line[-1][\"tweets_processed\"].append(tweet_id)\n",
    "        \n",
    "        fname_temp   = fname + \"_temp\"\n",
    "        fname_backup = fname + \"_backup\"\n",
    "        os.system(\"cp \" + fname + \" \" + fname_backup)\n",
    "        with open(fname_temp, \"w\") as fid:\n",
    "          fid.write(str(line))\n",
    "        # end with open\n",
    "        os.system(\"mv \" + fname_temp + \" \" + fname)\n",
    "      # end for jj\n",
    "    # end for ii\n",
    "\n",
    "    print(\"success process_all_tweets_by_user\")\n",
    "  # end process_all_tweets_by_user\n",
    "\n",
    "  #=====================================================\n",
    "  #=====================================================\n",
    "  #=====================================================\n",
    "\n",
    "  async def fetch_all_tweets_by_user(self, user_id):\n",
    "    print(\"begin fetch_all_tweets_by_user\")\n",
    "\n",
    "    username = self.special_tweeters[user_id]\n",
    "    fname = self.data_dir + \"/activity_\" + username + \".json\"\n",
    "\n",
    "    last_tweet_time = 0\n",
    "    if os.path.exists(fname) and \\\n",
    "        os.stat(fname).st_size != 0:\n",
    "      try:\n",
    "        with open(fname, \"r\") as fid:\n",
    "          activity = ast.literal_eval(fid.read())\n",
    "        # end with\n",
    "      except:\n",
    "        print(\"exception triggered when trying to load activity\")\n",
    "        print(\"now we're trying to load the backup.\")\n",
    "        with open(fname + \"_backup\", \"r\") as fid:\n",
    "          activity = ast.literal_eval(fid.read())\n",
    "        # end with open\n",
    "        print(\"we loaded the backup (fatbu activity) so now we'll re-set the file with the backup\")\n",
    "        os.system(\"cp \" + fname + \"_backup \" + \\\n",
    "                    fname)\n",
    "      # end try/except\n",
    "      await asyncio.sleep(0.1)\n",
    "\n",
    "      if \"last_tweet_time_s\" not in activity[0].keys() or \\\n",
    "         \"last_tweet_time_s\" not in activity[-1].keys():\n",
    "        for tweet_dict in activity:\n",
    "          print(\"tweet_dict: \", tweet_dict)\n",
    "          keys = tweet_dict.keys()\n",
    "          for key in keys:\n",
    "            if \"last_tweet_time_s\" == key:\n",
    "              continue\n",
    "            # end if\n",
    "\n",
    "            tweet_time = self.get_tweet_time_s(tweet_dict[key][\"tweet_created_at\"])\n",
    "            last_tweet_time = max(last_tweet_time, tweet_time)\n",
    "          # end for\n",
    "        # end for\n",
    "        for tweet_dict in activity:\n",
    "          tweet_dict[\"last_tweet_time_s\"] = last_tweet_time\n",
    "        # end for    \n",
    "      # end if\n",
    "      last_tweet_time = max(last_tweet_time, \n",
    "          activity[0][\"last_tweet_time_s\"], activity[-1][\"last_tweet_time_s\"])\n",
    "    # end if\n",
    "\n",
    "    fname = self.data_dir + \"/TweetsByUser\" + \"_\" + user_id + \".txt\"\n",
    "    print(\"fname: \", fname)\n",
    "\n",
    "    line = self.safe_load(fname)\n",
    "    await asyncio.sleep(0.1)\n",
    "\n",
    "    if \"last_tweet_time_s\" in line[0].keys():\n",
    "      last_tweet_time = max(last_tweet_time, line[0][\"last_tweet_time_s\"])\n",
    "      if \"last_tweet_time_s\" in line[-1].keys():\n",
    "        last_tweet_time = max(last_tweet_time, line[-1][\"last_tweet_time_s\"])\n",
    "      # end if\n",
    "    else:\n",
    "      for ii in range(len(line)):\n",
    "        for jj in range(len(line[ii][\"data\"])):\n",
    "          tweet_time = self.get_tweet_time_s(line[ii][\"data\"][jj][\"created_at\"])\n",
    "          last_tweet_time = max(last_tweet_time, tweet_time)\n",
    "        # end for jj\n",
    "      # end for ii\n",
    "      line[-1][\"last_tweet_time_s\"] = last_tweet_time\n",
    "      line[ 0][\"last_tweet_time_s\"] = last_tweet_time\n",
    "    # end if/else\n",
    "    await asyncio.sleep(0.1)\n",
    "\n",
    "    url = self.twitter_api_base[:-len(\"tweets/\")] + \"users/\" + user_id + \\\n",
    "          \"/tweets?tweet.fields=created_at&max_results=100\"\n",
    "\n",
    "    ## first we let it check for new tweets (possibly going all the way back)\n",
    "    url_og = url + \"\"\n",
    "    token = \"\"\n",
    "    loop  = True\n",
    "    num_loops = 0\n",
    "    newest_tweet_time = 0\n",
    "    while loop and num_loops < self.max_loops:\n",
    "      print(\"\\n\\nsleeping in process_all_tweets_by_user while loop\\n\\n\")\n",
    "      await asyncio.sleep(0.1) # otherwise Cntrl-C alwyas just kills the curl :(\n",
    "      print(\"num_loops: \", num_loops)\n",
    "      num_loops += 1\n",
    "\n",
    "      os.system(\"rm -f temp.txt\")\n",
    "      await self.save_url_to_file(url, \"temp.txt\")\n",
    "\n",
    "      if os.stat(\"temp.txt\").st_size == 0:\n",
    "        print(\"error, didn't grab any data, probably url has a bug\")\n",
    "        raise\n",
    "      # end if\n",
    "\n",
    "      with open(\"temp.txt\", \"r\") as fid:\n",
    "        line = fid.read()\n",
    "      # end with\n",
    "      line = ast.literal_eval(line)\n",
    "      print(line.keys())\n",
    "\n",
    "      recent_tweet_time = self.get_tweet_time_s(line[\"data\"] [0][\"created_at\"])\n",
    "      oldest_tweet_time = self.get_tweet_time_s(line[\"data\"][-1][\"created_at\"])\n",
    "\n",
    "      if oldest_tweet_time > recent_tweet_time: # switch 'em! (+0 ensures pass by val not by ref)\n",
    "        temp = oldest_tweet_time + 0\n",
    "        oldest_tweet_time = recent_tweet_time + 0\n",
    "        recent_tweet_time = temp + 0\n",
    "      # end if\n",
    "\n",
    "      newest_tweet_time = max(last_tweet_time, recent_tweet_time, newest_tweet_time)\n",
    "\n",
    "      print(\"last_tweet_time: \", last_tweet_time)\n",
    "      print(\"recent_tweet_time: \", recent_tweet_time)\n",
    "      print(\"oldest_tweet_time: \", oldest_tweet_time)\n",
    "      print(\"newest_tweet_time: \", newest_tweet_time)\n",
    "      #input(\">>\")\n",
    "\n",
    "      if recent_tweet_time <= last_tweet_time and oldest_tweet_time < last_tweet_time:\n",
    "        print(\"recent_tweet_time older than (or equal to) last saved tweet and oldest_tweet is too! Done here\")\n",
    "        break\n",
    "      # end if\n",
    "\n",
    "      with open(fname, \"r\") as fid:\n",
    "        line_old = fid.read()\n",
    "      # end with\n",
    "      line_old = ast.literal_eval(line_old)\n",
    "\n",
    "      if recent_tweet_time > last_tweet_time: # add tweets to file\n",
    "        ## note that it's okay if we repeat some tweets since we check tweet_ids to\n",
    "        ## avoid double processing later\n",
    "        line_old[-1][\"last_tweet_time_s\"] = newest_tweet_time\n",
    "        line_old[ 0][\"last_tweet_time_s\"] = newest_tweet_time\n",
    "        new_line = \"[\" + str(line) + \",\" + str(line_old)[1:]\n",
    "\n",
    "        with open(fname, \"w\") as fid:\n",
    "          fid.write(new_line)\n",
    "        # end with open\n",
    "      # end if\n",
    "\n",
    "      await asyncio.sleep(0.1)\n",
    "      if oldest_tweet_time < last_tweet_time:\n",
    "        print(\"oldest_tweet_time older than last tweet, time to break!\")\n",
    "        break\n",
    "      # end if\n",
    "\n",
    "      # otherwise keep going, find next_token\n",
    "      ## first delete temp.txt\n",
    "\n",
    "      os.system(\"rm -f temp.txt\")\n",
    "\n",
    "      if \"next_token\" in line[\"meta\"].keys():\n",
    "        token = line[\"meta\"][\"next_token\"]\n",
    "        url = url_og + \"&pagination_token=\" + token\n",
    "      else:\n",
    "        print(\"next_token not in line!\")\n",
    "        loop = False\n",
    "        break\n",
    "      # end if/else\n",
    "    # end while\n",
    "\n",
    "    ## then we check for old tweets\n",
    "    ## first, grab the oldest tweet id\n",
    "    with open(fname, \"r\") as fid:\n",
    "      line = fid.read()\n",
    "    # end with\n",
    "    line = ast.literal_eval(line)\n",
    "\n",
    "    oldest_line_num_ii = int(1e30)\n",
    "    oldest_line_num_jj = int(1e30)\n",
    "    oldest_tweet_id    = int(1e30)\n",
    "\n",
    "    for ii in range(len(line)):\n",
    "      for jj in range(len(line[ii][\"data\"])):\n",
    "        tweet_id = int(float(line[ii][\"data\"][jj][\"id\"]))\n",
    "        if tweet_id < oldest_tweet_id:\n",
    "          oldest_line_num_ii = ii\n",
    "          oldest_line_num_jj = jj\n",
    "        # end if\n",
    "      # end for jj\n",
    "    # end for ii\n",
    "    await asyncio.sleep(0.1)\n",
    "\n",
    "    ## now we do a while loop\n",
    "    url = self.twitter_api_base[:-len(\"tweets/\")] + \"users/\" + user_id + \\\n",
    "          \"/tweets?tweet.fields=created_at&max_results=100\"\n",
    "    url_og = url + \"\"\n",
    "    token = \"\"\n",
    "    loop  = True\n",
    "    num_loops = 0\n",
    "    newest_tweet_time = 0\n",
    "\n",
    "    ## check if it exists first!\n",
    "    if \"next_token\" in line[oldest_line_num_ii][\"meta\"].keys():\n",
    "      token = line[oldest_line_num_ii][\"meta\"][\"next_token\"]\n",
    "      url = url_og + \"&pagination_token=\" + token\n",
    "\n",
    "      while loop and num_loops < self.max_loops:\n",
    "        await asyncio.sleep(0.1) # otherwise Cntrl-C alwyas just kills the curl :(\n",
    "        print(\"num_loops: \", num_loops)\n",
    "        num_loops += 1\n",
    "\n",
    "        await self.save_url_to_file(url, fname)\n",
    "\n",
    "        if os.stat(fname).st_size == 0:\n",
    "          print(\"error, didn't grab any data, probably url has a bug\")\n",
    "          raise\n",
    "        # end if\n",
    "\n",
    "        # otherwise keep going, find next_token\n",
    "        ## first delete temp.txt\n",
    "\n",
    "        with open(fname, \"r\") as fid:\n",
    "          line = fid.read()\n",
    "        # end with open\n",
    "        line = ast.literal_eval(line)\n",
    "        \n",
    "        if \"next_token\" in line[-1][\"meta\"].keys():\n",
    "          token = line[\"meta\"][\"next_token\"]\n",
    "          url = url_og + \"&pagination_token=\" + token\n",
    "        else:\n",
    "          print(\"next_token not in line!\")\n",
    "          loop = False\n",
    "          break\n",
    "        # end if/else\n",
    "      # end while\n",
    "    # end if\n",
    "\n",
    "    os.system(\"rm -f temp.txt\")\n",
    "    print(\"success fetch_all_tweets_by_user\")\n",
    "  # end fetch_all_tweets_by_user\n",
    "\n",
    "  #=====================================================\n",
    "  #=====================================================\n",
    "  #=====================================================\n",
    "\n",
    "  def process_url_activity(self):\n",
    "    print(\"begin process_url_activity\")\n",
    "\n",
    "    fs = glob.glob(self.data_dir + \"/activity_*.txt\")\n",
    "    for fname in fs:\n",
    "      with open(fname, \"r\") as fid:\n",
    "        for line in fid:\n",
    "          pass\n",
    "        # end for line\n",
    "      # end with open\n",
    "      activity_by_url = ast.literal_eval(line)\n",
    "      tweet_id = list(activity_by_url.keys())[0]\n",
    "      tweet_creation_time = activity_by_url[tweet_id][\"tweet_created_at\"]\n",
    "\n",
    "      for dtype in self.dtypes:\n",
    "        data = activity_by_url[tweet_id][dtype]\n",
    "        for ii,user_id in enumerate(data[\"ids\"]):\n",
    "          if user_id not in list(self.activity_by_user.keys()):\n",
    "            self.activity_by_user[user_id] = \\\n",
    "              {\"usernames\": [],\n",
    "               \"num_keyword_replies\": 0,\n",
    "               \"num_keyword_retweets\": 0,\n",
    "               \"tweet_ids\": [],\n",
    "               \"tweet_contents\": [],\n",
    "               \"tweet_creation_times\": [],\n",
    "               \"Likes\": {\"num_Likes\": 0, \"tweet_ids\": [], \"tweet_creation_times\": []},\n",
    "               \"Retweets\": {\"num_Retweets\": 0, \"tweet_ids\": [], \"tweet_creation_times\": []},\n",
    "               \"QuoteTweets\": {\"num_QuoteTweets\": 0, \"tweet_ids\": [], \"tweet_creation_times\": [], \"tweet_contents\": []},\n",
    "               \"Replies\": {\"num_Replies\": 0, \"tweet_ids\": [], \"tweet_creation_times\": [], \"tweet_contents\": []}\n",
    "              }\n",
    "          # end if\n",
    "          if tweet_id not in list(self.activity_by_user[user_id][\"tweet_ids\"]):\n",
    "            self.activity_by_user[user_id][\"tweet_ids\"].append(tweet_id)\n",
    "          # end if\n",
    "          self.activity_by_user[user_id][\"usernames\"].append(self.user_dict[\"userId_to_username\"][user_id])\n",
    "\n",
    "          if dtype not in list(self.activity_by_user[user_id].keys()):\n",
    "            self.activity_by_user[user_id][dtype] = {\"num_\"+dtype: 1, \"tweet_ids\":[tweet_id], \"tweet_creation_times\":[tweet_creation_time]}\n",
    "            if dtype in [\"QuoteTweets\", \"Replies\"]:\n",
    "              self.activity_by_user[user_id][dtype][\"tweet_contents\"] = [data[\"contents\"][ii]]\n",
    "              self.activity_by_user[user_id][dtype][\"tweet_ids\"] = [data[\"tweet_ids\"][ii]]\n",
    "            # end if\n",
    "          else:\n",
    "            if tweet_id not in list(self.activity_by_user[user_id][dtype][\"tweet_ids\"]):\n",
    "              print(\"user_id: \", user_id)\n",
    "              print(\"act keys: \", self.activity_by_user[user_id].keys())\n",
    "              print(\"dtype: \", dtype)\n",
    "              self.activity_by_user[user_id][dtype][\"num_\"+dtype] += 1\n",
    "              self.activity_by_user[user_id][dtype][\"tweet_ids\"].append(tweet_id)\n",
    "              self.activity_by_user[user_id][dtype][\"tweet_creation_times\"].append(tweet_creation_time)\n",
    "              if dtype in [\"QuoteTweets\", \"Replies\"]:\n",
    "                self.activity_by_user[user_id][dtype][\"tweet_contents\"].append(data[\"contents\"][ii])\n",
    "                self.activity_by_user[user_id][dtype][\"tweet_ids\"][-1] = data[\"tweet_ids\"][ii]\n",
    "              # end if\n",
    "            # end if\n",
    "          # end if/else\n",
    "        # end for user_ids\n",
    "      # end for dtypes\n",
    "      fname2 = fname.split(\"_\")\n",
    "      fname2 = fname2[0] + \"_\" + fname2[1] + \"_\" + fname2[2] + \".json\"\n",
    "\n",
    "      line2 = \"\"\n",
    "\n",
    "      if os.path.exists(fname2) and \\\n",
    "       os.stat(fname2).st_size != 0:\n",
    "        with open(fname2, \"r\") as fid:\n",
    "          line2 = fid.read()\n",
    "        # end with\n",
    "      # end if\n",
    "\n",
    "      if line not in line2:\n",
    "        with open(fname2, \"w\") as fid:\n",
    "          if len(line2) == len(\"\"):\n",
    "            line = \"[\" + line + \"]\"\n",
    "          else:\n",
    "            line = line2[:-1] +\",\" + line + \"]\"\n",
    "          # end if/else\n",
    "          fid.write(line)\n",
    "        # end with\n",
    "      # end if\n",
    "      os.system(\"rm \" + fname)\n",
    "    # end for fnames\n",
    "\n",
    "    self.safe_save(self.fname_activity, self.activity_by_user)\n",
    "\n",
    "    print(\"success process_url_activity\")\n",
    "  # end process_url_activity\n",
    "\n",
    "  #=====================================================\n",
    "  #=====================================================\n",
    "  #=====================================================\n",
    "\n",
    "  async def update_keyword_data(self):\n",
    "    print(\"begin update_keyword_data\")\n",
    "    start = time.time()\n",
    "\n",
    "    dtime = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "    fname = self.data_dir + \"/\" + dtime + \"_keyword_data.txt\"\n",
    "    \n",
    "    query = self.keyword_query.replace(\" \", \"%20\")\n",
    "    url_og = \"https://api.twitter.com/2/tweets/search/recent?query=\" + query \\\n",
    "           + \"&user.fields=username&expansions=author_id&max_results=100\" \\\n",
    "           + \"&tweet.fields=created_at\"\n",
    "\n",
    "    url = url_og + \"\"\n",
    "    token = \"\"\n",
    "    loop  = True\n",
    "    num_loops = 0\n",
    "    while loop and num_loops < self.max_loops:\n",
    "      await asyncio.sleep(0.1) # otherwise Cntrl-C alwyas just kills the curl :(\n",
    "      print(\"num_loops: \", num_loops)\n",
    "      num_loops += 1\n",
    "\n",
    "      await self.save_url_to_file(url, fname)\n",
    "\n",
    "      if os.stat(fname).st_size == 0:\n",
    "        print(\"error, didn't grab any data, probably url has a bug\")\n",
    "        raise\n",
    "      # end if\n",
    "\n",
    "      ## check if we grabbed all of them up to last tweet fetched or not\n",
    "      with open(fname, \"r\") as fid:\n",
    "        for line in fid:\n",
    "          #print(\"line1: \", line)\n",
    "          inds = [m.start() for m in re.finditer(self.created_text, line)]\n",
    "          latest_tweet = line[inds[-1] + len(self.created_text):].split('\"')[0]\n",
    "          latest_tweet_s = self.get_tweet_time_s(latest_tweet)\n",
    "\n",
    "          if latest_tweet_s < self.activity_by_user[\"latest_tweet_time_s\"]:\n",
    "            loop = False\n",
    "            break\n",
    "          # end if\n",
    "\n",
    "          inds = [m.start() for m in re.finditer(self.meta_text, line)]\n",
    "          line = line[inds[-1]:]\n",
    "          print(\"line2: \", line)\n",
    "          print(\"inds: \", inds)\n",
    "\n",
    "          substr = '\"next_token\":\"'\n",
    "          if substr in line:\n",
    "            ind = line.find(substr) + len(substr)\n",
    "            line = line[ind:]\n",
    "            token = line.split('\"')[0]\n",
    "            print(\"token: \", token)\n",
    "\n",
    "            url = url_og + \"&pagination_token=\" + token\n",
    "          else:\n",
    "            print(\"substr not in line!\")\n",
    "            print(\"substr: \", substr)\n",
    "            loop = False\n",
    "          # end if\n",
    "        # end for\n",
    "      # end with\n",
    "    # end while\n",
    "\n",
    "    self.activity_by_user[\"query_url\"] = url_og\n",
    "\n",
    "    self.safe_save(self.fname_activity, self.activity_by_user)\n",
    "\n",
    "    print(\"ukd executed in (s): \", time.time() - start)\n",
    "    print(\"success update_keyword_data\")\n",
    "  # end update_keyword_data\n",
    "\n",
    "  #=====================================================\n",
    "  #=====================================================\n",
    "  #=====================================================\n",
    "\n",
    "  def safe_save(self, fname, data):\n",
    "    fname_temp = fname + \"_temp\"\n",
    "    fname_backup = fname + \"_backup\"\n",
    "\n",
    "    os.system(\"cp \" + fname + \" \" + fname_backup)\n",
    "    with open(fname_temp, \"w\") as fid:\n",
    "      fid.write(str(data))\n",
    "    # end with open\n",
    "    os.system(\"mv \" + fname_temp + \" \" + fname)\n",
    "  # end safe_save\n",
    "\n",
    "  #=====================================================\n",
    "  #=====================================================\n",
    "  #=====================================================\n",
    "\n",
    "  async def process_keyword_data(self):\n",
    "    print(\"begin process_keyword_data\")\n",
    "    start = time.time()\n",
    "\n",
    "    ## needs to be oldest to most recent\n",
    "    fs = np.sort(glob.glob(self.data_dir + \"/*_keyword_data.txt\"))\n",
    "\n",
    "    for fname in fs:\n",
    "      await asyncio.sleep(0.1)\n",
    "      print(\"fname: \", fname)\n",
    "      with open(fname, \"r\") as fid:\n",
    "        line = fid.read()\n",
    "      # end with open\n",
    "      try:\n",
    "        line = ast.literal_eval(line)\n",
    "      except:\n",
    "        line = \"[\" + line + \"]\"\n",
    "        line = line.replace(\"}{\", \"},{\")\n",
    "        line = ast.literal_eval(line)\n",
    "      # end try/except\n",
    "      if type(line) != type([]):\n",
    "        line = [line]\n",
    "      # end if\n",
    "\n",
    "      latest_tweet_s = 0.0\n",
    "      for ii in range(len(line)):\n",
    "        for jj in range(len(line[ii][\"data\"])):\n",
    "          tweet_time = self.get_tweet_time_s(line[ii][\"data\"][jj][\"created_at\"])\n",
    "          latest_tweet_s = max(latest_tweet_s, tweet_time)\n",
    "        # end for\n",
    "      # end for\n",
    "      await asyncio.sleep(0.02)\n",
    "\n",
    "      if self.activity_by_user[\"latest_tweet_time_s\"] >= latest_tweet_s:\n",
    "        print(\"skipping\")\n",
    "        os.system(\"rm \" + fname)\n",
    "        continue\n",
    "      # end if\n",
    "\n",
    "      user_ids  = []\n",
    "      usernames = []\n",
    "      for ii in range(len(line)):\n",
    "        for jj in range(len(line[ii][\"includes\"][\"users\"])):\n",
    "          user_ids.append(  line[ii][\"includes\"][\"users\"][jj][\"id\"])\n",
    "          usernames.append( line[ii][\"includes\"][\"users\"][jj][\"username\"])\n",
    "        # end for jj\n",
    "      # end for ii\n",
    "      await asyncio.sleep(0.02)\n",
    "\n",
    "      user_ids_to_usernames = {}\n",
    "      for ii in range(len(user_ids)):\n",
    "        user_ids_to_usernames[user_ids[ii]] = usernames[ii]\n",
    "      # end for ii\n",
    "      await asyncio.sleep(0.02)\n",
    "\n",
    "      tweet_ids = []\n",
    "      contents  = []\n",
    "      creations = []\n",
    "      author_ids = []\n",
    "\n",
    "      for ii in range(len(line)):\n",
    "        for jj in range(len(line[ii][\"data\"])):\n",
    "          tweet_ids.append( line[ii][\"data\"][jj][\"id\"])\n",
    "          contents.append(  line[ii][\"data\"][jj][\"text\"])\n",
    "          creations.append( line[ii][\"data\"][jj][\"created_at\"])\n",
    "          author_ids.append(line[ii][\"data\"][jj][\"author_id\"])\n",
    "        # end for\n",
    "      # end for\n",
    "      await asyncio.sleep(0.02)\n",
    "\n",
    "      latest_creation_time = np.sort(np.array(creations))[-1]\n",
    "      latest_creation_time_s = self.get_tweet_time_s(latest_creation_time)\n",
    "      self.activity_by_user[\"latest_tweet_time\"] = latest_creation_time\n",
    "      self.activity_by_user[\"latest_tweet_time_s\"] = latest_creation_time_s\n",
    "\n",
    "      for ii,author_id in enumerate(author_ids):\n",
    "        if author_id not in list(self.activity_by_user.keys()):\n",
    "          self.activity_by_user[author_id] = \\\n",
    "            {\"usernames\": [],\n",
    "             \"num_keyword_replies\": 0,\n",
    "             \"num_keyword_retweets\": 0,\n",
    "             \"tweet_ids\": [],\n",
    "             \"tweet_contents\": [],\n",
    "             \"tweet_creation_times\": []\n",
    "            }\n",
    "        # end if\n",
    "\n",
    "        if tweet_ids[ii] in self.activity_by_user[author_id]:\n",
    "          print(\"tweet_id in abu?\")\n",
    "          continue\n",
    "        # end if\n",
    "\n",
    "        if contents[ii][:2] == \"RT\":\n",
    "          self.activity_by_user[author_id][\"num_keyword_retweets\"] += 1\n",
    "        else:\n",
    "          self.activity_by_user[author_id][\"num_keyword_replies\"] += 1\n",
    "        # end if/else\n",
    "\n",
    "        self.activity_by_user[author_id][\"usernames\"].append(user_ids_to_usernames[author_id])\n",
    "        self.activity_by_user[author_id][\"tweet_ids\"].append(tweet_ids[ii])\n",
    "        self.activity_by_user[author_id][\"tweet_contents\"].append(contents[ii])\n",
    "        self.activity_by_user[author_id][\"tweet_creation_times\"].append(creations[ii])\n",
    "      # end for ii\n",
    "      await asyncio.sleep(0.02)\n",
    "\n",
    "      self.safe_save(self.fname_activity, self.activity_by_user)\n",
    "      os.system(\"rm \" + fname)\n",
    "    # end for fnames\n",
    "\n",
    "    print(\"pkd executed in (s): \", time.time() - start)\n",
    "    print(\"success process_keyword_data\")\n",
    "  # end process_keyword_data\n",
    "\n",
    "  #=====================================================\n",
    "  #=====================================================\n",
    "  #=====================================================\n",
    "  #=====================================================\n",
    "\n",
    "  async def fetch_user_leaderboard(self, start_time=\"2020-05-04T23:59:59.000Z\", \n",
    "    end_time=\"4022-05-04T23:59:59.000Z\", method=\"Points\", sharded=True):\n",
    "    print(\"begin fetch_user_leaderboard_sharded\")\n",
    "    tstart = time.time()\n",
    "    self.init_auth()\n",
    "\n",
    "    await asyncio.sleep(0.1)\n",
    "\n",
    "    start_time_s = self.get_tweet_time_s(start_time)\n",
    "    end_time_s   = self.get_tweet_time_s(end_time)\n",
    "\n",
    "    keyword_replies  = []\n",
    "    keyword_retweets = []\n",
    "    usernames = []\n",
    "    contents  = []\n",
    "    tweet_ids = []\n",
    "\n",
    "    num_dict = {\"num_Likes\":[],\n",
    "                \"num_Retweets\":[],\n",
    "                \"num_QuoteTweets\":[],\n",
    "                \"num_Replies\":[]}\n",
    "\n",
    "    for user in list(self.activity_by_user.keys()):\n",
    "      if sharded:\n",
    "        if user not in self.linked_userIds:\n",
    "          continue\n",
    "        # end if\n",
    "      else:\n",
    "        if user in [\"latest_tweet_time\", \"latest_tweet_time_s\", \"query_url\"]:\n",
    "          continue\n",
    "        # end if\n",
    "      # end if/else\n",
    "      print(\"user: \", user)\n",
    "      await asyncio.sleep(0.001)\n",
    "\n",
    "      tweet_times_dict = {}\n",
    "      for dtype in self.dtypes:\n",
    "        tweet_times_dict[dtype] = []\n",
    "        if dtype in self.activity_by_user[user].keys():\n",
    "          cnt = 0\n",
    "          for ii,tweet_time in enumerate(list(self.activity_by_user[user][dtype][\"tweet_creation_times\"])):\n",
    "            if tweet_time in tweet_times_dict[dtype]:\n",
    "              continue\n",
    "            # end if\n",
    "            tweet_times_dict[dtype].append(tweet_time)\n",
    "            tweet_ids.append(self.activity_by_user[user][dtype][\"tweet_ids\"][ii])\n",
    "\n",
    "            tweet_time = self.get_tweet_time_s(tweet_time)\n",
    "            if tweet_time >= start_time_s and tweet_time <= end_time_s:\n",
    "              cnt += 1\n",
    "            # end if\n",
    "          # end for\n",
    "          num_dict[\"num_\"+dtype].append(cnt)\n",
    "        else:\n",
    "          num_dict[\"num_\"+dtype].append(0)\n",
    "        # end if/else\n",
    "      # end for\n",
    "      #print(self.activity_by_user[user])\n",
    "      print(user)\n",
    "      replies_cnt = 0\n",
    "      retweets_cnt = 0\n",
    "      tweet_contents = []\n",
    "      for ii in range(len(self.activity_by_user[user][\"tweet_contents\"])):\n",
    "        ## avoiding double counting via looking at the tweet_ids from Likes etc\n",
    "        ## currently won't work since at the root level tweet_ids contains all tweet_ids\n",
    "        ## not just the ones from keyword stuff :(\n",
    "        ## so either I gotta live with double counting or kind of start over.\n",
    "        #if self.activity_by_user[user][\"tweet_ids\"][ii] in tweet_ids:\n",
    "          #continue\n",
    "        tweet_content = self.activity_by_user[user][\"tweet_contents\"][ii]\n",
    "        if tweet_content in tweet_contents:\n",
    "          continue\n",
    "        # end if\n",
    "        tweet_contents.append(tweet_content)\n",
    "        tweet_content = tweet_content.lower()\n",
    "\n",
    "        ## below if statement to exclude LuckyRooToken tweets...or we\n",
    "        ## could just check for whitelisted users???\n",
    "        if \"#luckyroo\" in tweet_content or \"@luckyr\" in tweet_content or \\\n",
    "           \"#saita\" in tweet_content or \"@saita\" in tweet_content or \\\n",
    "           \"promote it on\" in tweet_content: # last one to filter out spam bots\n",
    "          continue\n",
    "        # end if\n",
    "\n",
    "\n",
    "        tweet_time = self.activity_by_user[user][\"tweet_creation_times\"][ii]\n",
    "        tweet_time = self.get_tweet_time_s(tweet_time)\n",
    "        if tweet_time >= start_time_s and tweet_time <= end_time_s:\n",
    "          if self.activity_by_user[user][\"tweet_contents\"][ii][:3] == \"RT \":\n",
    "            retweets_cnt += 1\n",
    "          else:\n",
    "            replies_cnt += 1\n",
    "          # end if/else\n",
    "        # end if\n",
    "      # end for\n",
    "\n",
    "      keyword_replies.append(replies_cnt)\n",
    "      keyword_retweets.append(retweets_cnt)\n",
    "      usernames.append(self.activity_by_user[user][\"usernames\"][-1])\n",
    "      for content in list(self.activity_by_user[user][\"tweet_contents\"]):\n",
    "        contents.append(content)\n",
    "      # end for\n",
    "    # end for\n",
    "\n",
    "    keyword_replies = np.array(keyword_replies)\n",
    "    keyword_retweets = np.array(keyword_retweets)\n",
    "    num_Likes = np.array(num_dict[\"num_Likes\"])\n",
    "    num_Retweets = np.array(num_dict[\"num_Retweets\"])\n",
    "    num_QuoteTweets = np.array(num_dict[\"num_QuoteTweets\"])\n",
    "    num_Replies = np.array(num_dict[\"num_Replies\"])\n",
    "\n",
    "    points = keyword_replies*3 + num_Retweets*2 + keyword_retweets*2 + \\\n",
    "             num_Likes*1 + num_QuoteTweets*4 + num_Replies*3\n",
    "\n",
    "    usernames = np.array(usernames)\n",
    "\n",
    "    if   method == \"Points\":\n",
    "      inds = np.argsort(points)[::-1]\n",
    "      val = points\n",
    "    elif method == \"Likes\":\n",
    "      inds = np.argsort(num_Likes)[::-1]\n",
    "      val = num_Likes\n",
    "    elif method == \"Retweets\":\n",
    "      inds = np.argsort(num_Retweets)[::-1]\n",
    "      val = num_Retweets\n",
    "    elif method == \"QuoteTweets\":\n",
    "      inds = np.argsort(num_QuoteTweets)[::-1]\n",
    "      val = num_QuoteTweets\n",
    "    elif method == \"Replies\":\n",
    "      inds = np.argsort(num_Replies)[::-1]\n",
    "      val = num_Replies\n",
    "    elif method == \"keyword_replies\":\n",
    "      inds = np.argsort(keyword_replies)[::-1]\n",
    "      val = keyword_replies\n",
    "    elif method == \"keyword_retweets\":\n",
    "      inds = np.argsort(keyword_retweets)[::-1]\n",
    "      val = keyword_retweets\n",
    "    # end if/elifs\n",
    "\n",
    "    inds = inds[:20]\n",
    "    leaderboard = \"\"\n",
    "    for ii in range(len(inds)):\n",
    "      leaderboard += str(ii) + \") \" + usernames[inds][ii] + \": \" + str(val[inds][ii])\n",
    "      leaderboard += \"\\n\"\n",
    "      print(str(ii) + \") \" + usernames[inds][ii] + \": \", val[inds][ii])\n",
    "    # end for ii\n",
    "    print(\"num usernames: \", len(usernames))\n",
    "    print(\"num tweets: \", np.sum(keyword_replies) + np.sum(keyword_retweets) + \\\n",
    "          np.sum(num_dict[\"num_Replies\"]) + np.sum(num_dict[\"num_QuoteTweets\"]))\n",
    "    print(\"method: \", method)\n",
    "\n",
    "    #for ii in range(20):\n",
    "    #  ind = np.random.randint(len(contents))\n",
    "    #  print(\"\\n\" + contents[ind])\n",
    "    ## end for ii\n",
    "\n",
    "    ## note, this errors sometimes with a utf-8 issue so I guess\n",
    "    ## sometimes it gets passes some dumb emoji :)\n",
    "    #with open(self.data_dir + \"/queryed_tweets.txt\", \"w\") as fid:\n",
    "    #  for ii in range(len(contents)):\n",
    "    #    fid.write(contents[ii] + \"\\n\\n\")\n",
    "      # end for line\n",
    "    # end with open\n",
    "\n",
    "    await asyncio.sleep(0.03)\n",
    "    fname = self.data_dir + \"/leaderboardSharded_\" + method + \"_start\" + \\\n",
    "            start_time + \"_\" + end_time + \".txt\"\n",
    "    tnow = datetime.datetime.now()\n",
    "    tnow = tnow.strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "    with open(fname, \"w\") as fid:\n",
    "      fid.write(\"last updated: \" + tnow + \"\\n\")\n",
    "      fid.write(leaderboard)\n",
    "    # end with\n",
    "\n",
    "    print(\"fuls executed in (s): \", time.time() - tstart)\n",
    "    print(\"success fetch_user_leaderboard_sharded\")\n",
    "    return leaderboard\n",
    "  # end fetch_user_leaderboard_sharded\n",
    "\n",
    "  #=====================================================\n",
    "  #=====================================================\n",
    "  #=====================================================\n",
    "  #=====================================================\n",
    "\n",
    "  def verify_processed_tweet(self, tweet_url, username=\"\"):\n",
    "    self_tweet_id, self_creator_username = self.init_tweet(tweet_url)\n",
    "    print(\"tw_id: \", self_tweet_id)\n",
    "    print(\"cr_un: \", self_creator_username)\n",
    "    \n",
    "    if self_creator_username not in self.special_tweeters:\n",
    "      username = self_creator_username\n",
    "    # end if\n",
    "\n",
    "    if username == \"\":\n",
    "      error_msg  = \"error! Didn't supply username and we were asked to verify\"\n",
    "      error_msg += \" interaction with @RooTroopNFT or @TroopSales\"\n",
    "      return [error_msg, False]\n",
    "    # end if\n",
    "\n",
    "    status, user_data = self.fetch_user_data(username)\n",
    "\n",
    "    if status == False:\n",
    "      return [\"error! username isn't linked??\", False]\n",
    "    # end if\n",
    "\n",
    "    if self_tweet_id in user_data[\"tweet_ids\"]:\n",
    "      message = \"SUCCESS! Your tweet was already processed :)\"\n",
    "      print(message)\n",
    "      return [message,True]\n",
    "    # end if\n",
    "        \n",
    "    url = self.twitter_api_base[:-1] + \"?ids=\" + self_tweet_id \\\n",
    "        + \"&tweet.fields=created_at\"\n",
    "\n",
    "    os.system(self.curl_base + url + self.curl_header + self.auth + \n",
    "              \"' > \" + \"delete_me.txt\")\n",
    "\n",
    "    with open(\"delete_me.txt\", \"r\") as fid:\n",
    "      line = fid.read()\n",
    "    # end with open\n",
    "\n",
    "    print(\"self created_text: \", self.created_text)\n",
    "    print(\"line: \", line)\n",
    "    print(\"line split self created_text: \", line.split(self.created_text))\n",
    "    tweet_time = line.split(self.created_text)[1].split('\"')[0]\n",
    "    tweet_time_s = self.get_tweet_time_s(tweet_time)\n",
    "\n",
    "    if self.activity_by_user[\"latest_tweet_time_s\"] < tweet_time_s:\n",
    "      message = \"This tweet created after last query was made\"\n",
    "      print(message)\n",
    "      return [message,False]\n",
    "    # end if\n",
    "\n",
    "    message = \"I have no idea what went wrong but there was an error.\"\n",
    "\n",
    "    print(message)\n",
    "    return [message,False]\n",
    "  # end verify_processed_tweet\n",
    "\n",
    "  #=====================================================\n",
    "  #=====================================================\n",
    "  #=====================================================\n",
    "\n",
    "  def fetch_user_data(self, username):\n",
    "    try:\n",
    "      for stored_username in self.user_dict[\"username_to_userId\"].keys():\n",
    "        if username.lower() == stored_username.lower():\n",
    "          user_id = self.user_dict[\"username_to_userId\"][stored_username]\n",
    "          return [True, self.activity_by_user[user_id]]\n",
    "        # end if\n",
    "      # end for\n",
    "      for key in list(self.activity_by_user.keys()):\n",
    "        if key[0].isdigit():\n",
    "          for stored_username in self.activity_by_user[key][\"usernames\"]:\n",
    "            if username.lower() == stored_username.lower():\n",
    "              self.user_dict[\"username_to_userId\"][stored_username] = key\n",
    "              self.user_dict[\"userId_to_username\"][key] = stored_username\n",
    "              return [True, self.activity_by_user[key]]\n",
    "            # end if\n",
    "          # end for\n",
    "        # end if\n",
    "      # end for\n",
    "      return [False, \"error! username not found :(\"]\n",
    "    except:\n",
    "      for key in list(self.activity_by_user.keys()):\n",
    "        if key[0].isdigit():\n",
    "          for stored_username in self.activity_by_user[key][\"usernames\"]:\n",
    "            if username.lower() == stored_username.lower():\n",
    "              self.user_dict[\"username_to_userId\"][stored_username] = key\n",
    "              self.user_dict[\"userId_to_username\"][key] = stored_username\n",
    "              return [True, self.activity_by_user[key]]\n",
    "            # end if\n",
    "          # end for\n",
    "        # end if\n",
    "      # end for\n",
    "      return [False, \"error! username not found :(\"]\n",
    "    # end try/except\n",
    "  # def fetch_user_data\n",
    "\n",
    "  #=====================================================\n",
    "  #=====================================================\n",
    "  #=====================================================\n",
    "\n",
    "  def fetch_user_points(self, user_data):\n",
    "    ## uses result of fetch_user_data above\n",
    "\n",
    "    num_dict = {\"num_Likes\":0,\n",
    "                \"num_Retweets\":0,\n",
    "                \"num_QuoteTweets\":0,\n",
    "                \"num_Replies\":0,\n",
    "                \"num_keyword_retweets\":0,\n",
    "                \"num_keyword_replies\":0}\n",
    "\n",
    "    tweet_times = {}\n",
    "    tweet_ids = []\n",
    "    for dtype in self.dtypes:\n",
    "      tweet_times[dtype] = []\n",
    "      if dtype in user_data.keys():\n",
    "        for ii,tweet_time in enumerate(user_data[dtype][\"tweet_creation_times\"]):\n",
    "          if tweet_time in tweet_times[dtype]:\n",
    "            continue\n",
    "          # end if\n",
    "          num_dict[\"num_\"+dtype] += 1\n",
    "          tweet_times[dtype].append(tweet_time)\n",
    "          tweet_ids.append(user_data[dtype][\"tweet_ids\"][ii])\n",
    "        # end for\n",
    "      # end if\n",
    "    # end for\n",
    "\n",
    "    keyword_replies  = 0\n",
    "    keyword_retweets = 0\n",
    "    tweet_contents = []\n",
    "    for ii,tweet_content in enumerate(user_data[\"tweet_contents\"]):\n",
    "      #if user_data[\"tweet_ids\"][ii] in tweet_ids:\n",
    "      #  continue\n",
    "      # end if\n",
    "      ## the above is to avoid double counting from the interactions with the keyword\n",
    "      ## stuff. No need to append to tweet_ids :)\n",
    "      ## But it doesn't work currently, see note in fetch_user_leaderboard... :*(\n",
    "\n",
    "      if tweet_content in tweet_contents:\n",
    "        continue\n",
    "      # end if\n",
    "      tweet_contents.append(tweet_content)\n",
    "      tweet_content = tweet_content.lower()\n",
    "\n",
    "      ## below if statement to exclude LuckyRooToken tweets...or we\n",
    "      ## could just check for whitelisted users???\n",
    "      if \"#luckyroo\" in tweet_content or \"@luckyr\" in tweet_content or \\\n",
    "         \"#saita\" in tweet_content or \"@saita\" in tweet_content or \\\n",
    "         \"promote it on\" in tweet_content: # last one to filter out spam bots\n",
    "        continue\n",
    "      # end if\n",
    "\n",
    "      if user_data[\"tweet_contents\"][ii][:3] == \"RT \":\n",
    "        keyword_retweets += 1\n",
    "      else:\n",
    "        keyword_replies += 1\n",
    "      # end if/else\n",
    "\n",
    "      num_dict[\"num_keyword_replies\"] = keyword_replies\n",
    "      num_dict[\"num_keyword_retweets\"] = keyword_retweets\n",
    "    # end for\n",
    "\n",
    "    points = num_dict[\"num_keyword_replies\" ]*3 + num_dict[\"num_Retweets\"]*2 \\\n",
    "           + num_dict[\"num_keyword_retweets\"]*2 + num_dict[\"num_Likes\"]*1 \\\n",
    "           + num_dict[\"num_QuoteTweets\"]*4 + num_dict[\"num_Replies\"]*3\n",
    "    \n",
    "    message  = \"Points: \" + str(points) + \"\\n\"\n",
    "    message += \"Likes: \" + str(num_dict[\"num_Likes\"]) + \"\\n\"\n",
    "    message += \"Retweets: \" + str(num_dict[\"num_Retweets\"]) + \"\\n\"\n",
    "    message += \"QuoteTweets: \" + str(num_dict[\"num_QuoteTweets\"]) + \"\\n\"\n",
    "    message += \"Replies: \" + str(num_dict[\"num_Replies\"]) + \"\\n\"\n",
    "    message += \"num_keyword_replies: \" + str(num_dict[\"num_keyword_replies\"]) + \"\\n\"\n",
    "    message += \"num_keyword_retweets: \" + str(num_dict[\"num_keyword_retweets\"]) + \"\\n\\n\"\n",
    "    message += \"Note that points are computed as Likes + Retweets*2 + keyword_retweets*2 + Replies*3 + keyword_replies*3 + QuoteTweets*4\\n\"\n",
    "    message += \"where Likes,Retweets,Replies,QuoteTweets are interactions with @RooTroopNFT and/or @TroopSales, while keyword_replies (misnomer, includes if you start the tweet) and keyword_retweets use the keyword search. Use 'rtt keywords' for more info on those.\\n\\n\"\n",
    "\n",
    "    print(\"SUCCESS fetch_user_points\")\n",
    "    return message\n",
    "  # end fetch_user_points\n",
    "\n",
    "  #=====================================================\n",
    "  #=====================================================\n",
    "  #=====================================================\n",
    "\n",
    "  def safe_load(self, fname):\n",
    "    ## first, load activity by user :D\n",
    "    result = {}\n",
    "    if os.path.exists(fname) and \\\n",
    "       os.stat(fname).st_size != 0:\n",
    "      try:\n",
    "        with open(fname, \"r\") as fid:\n",
    "          result = ast.literal_eval(fid.read())\n",
    "        # end with\n",
    "      except:\n",
    "        print(\"exception triggered when trying to load \" + fname + \"in safe load\")\n",
    "        print(\"now we're trying to load the backup.\")\n",
    "        with open(fname + \"_backup\", \"r\") as fid:\n",
    "          result = ast.literal_eval(fid.read())\n",
    "        # end with open\n",
    "        print(\"we loaded the backup (sl) so now we'll re-set the file with the backup2\")\n",
    "        os.system(\"cp \" + fname + \"_backup \" + \\\n",
    "                    fname)\n",
    "      # end try/except\n",
    "    # end if\n",
    "    return result\n",
    "  # end safe_load\n",
    "\n",
    "  #=====================================================\n",
    "  #=====================================================\n",
    "  #=====================================================\n",
    "\n",
    "  async def shard_data(self):\n",
    "    \"\"\"The purpose of this method is to break down activity_by_user.json\n",
    "    into one file per userid so that at least certain actions can be\n",
    "    done a lot quicker. Only sharding users that link in discord.\"\"\"\n",
    "    print(\"BEGIN shard_data\")\n",
    "\n",
    "    save_dir = self.data_dir + \"/user_data\"\n",
    "    os.system(\"mkdir -p \" + save_dir)\n",
    "\n",
    "    with open(\"discord_data/linked_3.json\", \"r\") as fid:\n",
    "      line = ast.literal_eval(fid.read())\n",
    "      linked_usernames = []\n",
    "      for el in line:\n",
    "        linked_usernames.append(el[\"handle\"])\n",
    "      # end for\n",
    "    # end with open\n",
    "    print(\"loaded linked_usernames!\")\n",
    "\n",
    "    for key in list(self.activity_by_user.keys()):\n",
    "      print(\"key: \", key)\n",
    "      if key[0].isdigit():\n",
    "        flag = False\n",
    "        for username in list(self.activity_by_user[key][\"usernames\"]):\n",
    "          for linked_username in linked_usernames:\n",
    "            if username.lower() == linked_username.lower():\n",
    "              flag = True\n",
    "              user_data = self.activity_by_user[key]\n",
    "              fname = save_dir + \"/\" + key + \".json\"\n",
    "              with open(fname, \"w\") as fid:\n",
    "                fid.write(str(user_data))\n",
    "              # end with open\n",
    "              break\n",
    "            # end if\n",
    "          # end for linked_usernames\n",
    "          if flag:\n",
    "            flag = False\n",
    "            break\n",
    "          # end if\n",
    "        # end for usernames\n",
    "      # end if\n",
    "      await asyncio.sleep(0.2)\n",
    "    # end for keys\n",
    "    print(\"SUCCESS shard_data\")\n",
    "  # end def shard_data\n",
    "\n",
    "  async def remove_duplicates(self):\n",
    "    for userId in list(self.activity_by_user.keys()):\n",
    "      if not userId[0].isdigit():\n",
    "        continue\n",
    "      # end if\n",
    "\n",
    "      usernames = []\n",
    "      for username in list(self.activity_by_user[userId][\"usernames\"]):\n",
    "        if username not in usernames:\n",
    "          usernames.append(username)\n",
    "        # end if\n",
    "      # end for\n",
    "      self.activity_by_user[userId][\"usernames\"] = usernames\n",
    "\n",
    "      ## there are other duplicates I could remove although \n",
    "      ## the calculate points things handle them so no real need...\n",
    "\n",
    "      print(\"\")\n",
    "\n",
    "  #=====================================================\n",
    "  #=====================================================\n",
    "  #=====================================================\n",
    "\n",
    "  async def continuously_scrape(self):\n",
    "    import random\n",
    "    print(\"hello from: \", random.random()*1e3)\n",
    "    await asyncio.sleep(1)\n",
    "    self.init_auth()\n",
    "    cs_start = time.time()\n",
    "    reset_time = time.time() - 400\n",
    "\n",
    "    while True:\n",
    "      await asyncio.sleep(0.1)\n",
    "      loop_start = time.time()\n",
    "      await self.update_keyword_data()\n",
    "      await asyncio.sleep(0.1)\n",
    "      await self.process_keyword_data()\n",
    "\n",
    "      if time.time() - reset_time > 5*60:\n",
    "        ## cs_user_id to avoid naming collisions\n",
    "        for cs_user_id in self.special_tweeters.keys():\n",
    "          if self.special_tweeters[cs_user_id] == \"troopsales\":\n",
    "            continue\n",
    "          # end if\n",
    "          print(cs_user_id)\n",
    "          await self.process_all_tweets_by_user(cs_user_id, update=True)\n",
    "        # end for\n",
    "        reset_time = time.time()\n",
    "      # end if\n",
    "\n",
    "      msg1 = \"last loop executed in: \" + str(time.time() - loop_start)\n",
    "      msg2 = \"time since continuous scraping started: \" + str(time.time() - cs_start)\n",
    "      msg3 = \"current date/time: \" + str(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M%S\"))\n",
    "      print(\"\\n\\n\\n\\n\\n\\n\\n\\n\" + msg1)\n",
    "      print(msg2)\n",
    "      print(msg3 + \"\\n\\n\\n\\n\\n\\n\\n\\n\")\n",
    "\n",
    "      fname = self.data_dir + \"/continuous_scrape_times.txt\"\n",
    "      with open(fname, \"a\") as fid:\n",
    "        fid.write(msg1 + \"\\n\")\n",
    "        fid.write(msg2 + \"\\n\")\n",
    "        fid.write(msg3 + \"\\n\")\n",
    "      # end with\n",
    "\n",
    "      #break\n",
    "      await asyncio.sleep(self.continuously_scrape_sleep_time)\n",
    "    # end while\n",
    "  # end def continuously_scrape\n",
    "\n",
    "  #=====================================================\n",
    "  #=====================================================\n",
    "  #=====================================================\n",
    "\n",
    "  def discord_bot(self):\n",
    "    print(\"begin discord_bot\")\n",
    "    import discord\n",
    "    client = discord.Client()\n",
    "    print(\"client loaded\")\n",
    "\n",
    "    ## bot would like read messages/view channels + send messages + read msg history permissions: 68608\n",
    "    # for ToTheMoons\n",
    "    BOT_COMMANDS_CID = 932056137518444594\n",
    "\n",
    "    @client.event\n",
    "    async def on_ready():\n",
    "      print(\"We have logged in as {0.user}\".format(client))\n",
    "      channel = client.get_channel(BOT_COMMANDS_CID)\n",
    "      await channel.send(\"I AM ALIVE! MWAHAHAHA\")\n",
    "      #await self.continuously_scrape()\n",
    "    # end\n",
    "\n",
    "    @client.event\n",
    "    async def on_message(message):\n",
    "      channel = client.get_channel(message.channel.id)\n",
    "\n",
    "      msg = message.content.lower()\n",
    "      if msg.startswith(\"rtt\"):\n",
    "        edit = False\n",
    "\n",
    "        if \"help\" in msg or \"halp\" in msg:\n",
    "          msg2  = \"Hi! I'm Twitteroo, developed by ryanjsfx.eth\\n\"\n",
    "          msg2 += \"AKA TheLunaLabs.eth, copyright 2022\\n.\"\n",
    "\n",
    "          if \"lb\" in msg or \"leaderboard\" in msg:\n",
    "            # expand on leaderboard options\n",
    "            msg2 += \"  options: rtt lb like (likes leaderboard),\\n\"\n",
    "            msg2 += \"  options: rtt lb retweet (retweets leaderboard),\\n\"\n",
    "            msg2 += \"    alt:   rtt lb rt\\n\"\n",
    "            msg2 += \"  options: rtt lb QuoteTweets (QuoteTweets leaderboard),\\n\"\n",
    "            msg2 += \"    alt:   rtt lb qt\\n\"\n",
    "            msg2 += \"  options: rtt lb Replies (Replies leaderboard),\\n\"\n",
    "            msg2 += \"    alt:   rtt lb reply\\n\"\n",
    "            msg2 += \"  options: rtt lb keywords (keywords leaderboard),\\n\"\n",
    "            msg2 += \"    alt:   rtt lb key\\n\\n\"\n",
    "            msg2 += \"you can also use the following time commands (warning, unkown time commands will just use all data)\\n\"\n",
    "            msg2 += \"  today (alt == 24H): past 24 hours (time-zone agnostic)\\n\"\n",
    "            msg2 += \"  Q1: data from 2022-01-01,0:0:0 to 2022-04-01,0:0:0\\n\"\n",
    "            msg2 += \"  Q2: data from 2022-04-01,0:0:0 to 2022-07-01,0:0:0\\n\"\n",
    "            msg2 += \"  last year (alt==2021): data from 2021-01-01,0:0:0 to 2022-01-01,0:0:0\\n\"\n",
    "            msg2 += \"  last month (alt==april): data from 2022-04-01,0:0:0 to 2022-05-01,0:0:0\\n\"\n",
    "            msg2 += \"  month (alt==may): data from 2022-05-01,0:0:0 to 2022-06-01,0:0:0\\n\"\n",
    "            msg2 += \"  Jan: data from 2022-01-01,0:0:0 to 2022-02-01,0:0:0\\n\"\n",
    "            msg2 += \"  Feb: data from 2022-02-01,0:0:0 to 2022-03-01,0:0:0\\n\"\n",
    "            msg2 += \"  Mar: data from 2022-03-01,0:0:0 to 2022-04-01,0:0:0\\n\"\n",
    "            msg2 += \"  start:blah, end:blah == user defined. Must fit expected style and spaces matter! Example: start:2022-01-01Z13:45:51.000Z, end:2022-03-03Z03:33:33.000Z\"\n",
    "          else:\n",
    "            # print a minimal list of commands.\n",
    "            msg2 += \"Here are my commands\\n\"\n",
    "            msg2 += \"which are case insensitive :)\\n\"\n",
    "            msg2 += \"rtt help: display this command ;)\\n\"\n",
    "            msg2 += \"  alts: rtt halp\\n\\n\"\n",
    "            msg2 += \"rtt lb: display leaderboard for given command options\\n\"\n",
    "            msg2 += \"  alts: rtt leabderboard\\n\"\n",
    "            msg2 += \"  defaults: points leaderboard, all data\\n\"\n",
    "            msg2 += \"  more info: rtt help lb\\n\\n\"\n",
    "            msg2 += \"rtt keywords: display keywords we use to find tweets\\n\"\n",
    "            msg2 += \"rtt verify: verify if we've processed your tweet/interaction\\n\"\n",
    "            msg2 += \"  usage: rtt verify url:blah, username:blah\\n\"\n",
    "            msg2 += \"rtt me: display user's points, likes, etc.\\n\"\n",
    "            msg2 += \"  usage: rtt me username:blah\\n\"\n",
    "          # end if/else\n",
    "\n",
    "        elif msg.startswith(\"rtt me\"):\n",
    "          username = \"\"\n",
    "          try:\n",
    "            username  = msg.split(\"username:\")[1].replace(\" \",\"\")\n",
    "          except:\n",
    "            msg2  = \"sorry, I couldn't parse that. I'm loooking for smtg like\\n\"\n",
    "            msg2 += \"rtt me username:TheLunaLabs\"\n",
    "          # end try/except\n",
    "          if username != \"\":\n",
    "            self.init_auth()\n",
    "            await channel.send(\"fetching user data\")\n",
    "            status, user_data = self.fetch_user_data(username)\n",
    "            if status == False:\n",
    "              msg2 = user_data\n",
    "            else:\n",
    "              #print(user_data)\n",
    "              await channel.send(\"and now computing user points\")\n",
    "              msg2 = self.fetch_user_points(user_data)\n",
    "            # end if/else\n",
    "          # end if\n",
    "\n",
    "        elif \"verify\" in msg:\n",
    "          username = \"\"\n",
    "          try:\n",
    "            tweet_url = msg.split(\"url:\")[1]\n",
    "            if \"username:\" in msg:\n",
    "              tweet_url = tweet_url.split(\"username:\")[0]\n",
    "              username  = msg.split(\"username:\")[1]\n",
    "              username  = username.replace(\",\", \"\").replace(\" \", \"\")\n",
    "              print(\"username: \", username)\n",
    "            # end if\n",
    "            tweet_url = tweet_url.replace(\",\", \"\").replace(\" \", \"\")\n",
    "            if \"?\" in tweet_url:\n",
    "              tweet_url = tweet_url.split(\"?\")[0]\n",
    "            # end if\n",
    "          except:\n",
    "            msg2  = \"sorry, I couldn't parse that. I'm loooking for smtg like\\n\"\n",
    "            msg2 += \"rtt verify url:https://twitter.com/RooTroopNFT/status/1499858580568109058, username:TheLunaLabs\"\n",
    "          # end try/except\n",
    "          self.init_auth()\n",
    "          await channel.send(\"okay! will verify if we processed that tweet for that user yet or not\")\n",
    "          print(\"tweet_url: \", tweet_url)\n",
    "          msg2,status = self.verify_processed_tweet(tweet_url, username)\n",
    "\n",
    "        elif \"lb\" in msg or \"leaderboard\" in msg:\n",
    "          method = \"Points\"\n",
    "          if \"like\" in msg:\n",
    "            method = \"Likes\"\n",
    "          elif \"rt\" in msg[3:] or \"retweet\" in msg:\n",
    "            method = \"Retweets\"\n",
    "          elif \"qt\" in msg or \"quote\" in msg:\n",
    "            method = \"QuoteTweets\"\n",
    "          elif \"repl\" in msg:\n",
    "            method = \"Replies\"\n",
    "          elif \"key\" in msg:\n",
    "            method = \"keyword_replies\"\n",
    "          # end if/elifs\n",
    "\n",
    "          start_time = \"2020-05-04T23:59:59.000Z\"\n",
    "          end_time   = \"4022-05-04T23:59:59.000Z\"\n",
    "          tnow = datetime.datetime.now()\n",
    "          tstr = \"%Y-%m-%dT%H:%M:%S.000Z\"\n",
    "          time_str = \"all\"\n",
    "          if \"today\" in msg or \"24H\" in msg:\n",
    "            ## to be time-zone agnostic we start 24H ago and end now\n",
    "            start_time = (tnow - datetime.timedelta(days=1)).strftime(tstr)\n",
    "            end_time = tnow.strftime(tstr)\n",
    "            time_str = \"past 24 hours\"\n",
    "          elif \"Q1\" in msg:\n",
    "            start_time = \"2022-01-01T00:00:00.000Z\"\n",
    "            end_time   = \"2022-04-01T00:00:00.000Z\"\n",
    "            time_str = \"Q1 (2022)\"\n",
    "          elif \"Q2\" in msg:\n",
    "            start_time = \"2022-04-01T00:00:00.000Z\"\n",
    "            end_time   = \"2022-07-01T00:00:00.000Z\"\n",
    "            time_str = \"Q2 (2022)\"\n",
    "          elif \"last year\" in msg or \"2021\" in msg:\n",
    "            start_time = \"2021-01-01T00:00:00.000Z\"\n",
    "            end_time   = \"2022-01-01T00:00:00.000Z\"\n",
    "            time_str = \"2021\"\n",
    "          elif \"year\" in msg or \"2022\" in msg:\n",
    "            start_time = \"2022-01-01T00:00:00.000Z\"\n",
    "            end_time   = \"2023-01-01T00:00:00.000Z\"\n",
    "            time_str = \"2022\"\n",
    "          elif \"last month\" in msg or \"april\" in msg:\n",
    "            start_time = \"2022-04-01T00:00:00.000Z\"\n",
    "            end_time   = \"2022-05-01T00:00:00.000Z\"\n",
    "            time_str = \"April 2022\"\n",
    "          elif \"month\" in msg or \"may\" in msg:\n",
    "            start_time = \"2022-05-01T00:00:00.000Z\"\n",
    "            end_time   = \"2022-06-01T00:00:00.000Z\"\n",
    "            time_str = \"May 2022\"\n",
    "          elif \"jan\" in msg:\n",
    "            start_time = \"2022-01-01T00:00:00.000Z\"\n",
    "            end_time   = \"2022-02-01T00:00:00.000Z\"\n",
    "            time_str = \"Jan 2022\"\n",
    "          elif \"feb\" in msg:\n",
    "            start_time = \"2022-02-01T00:00:00.000Z\"\n",
    "            end_time   = \"2022-03-01T00:00:00.000Z\"\n",
    "            time_str = \"Feb 2022\"\n",
    "          elif \"mar\" in msg:\n",
    "            start_time = \"2022-03-01T00:00:00.000Z\"\n",
    "            end_time   = \"2022-04-01T00:00:00.000Z\"\n",
    "            time_str = \"Mar 2022\"\n",
    "          elif \"start:\" in msg and \", end:\" in msg:\n",
    "            start_time = msg.split(\"start:\")[1].split(\", end:\")[0]\n",
    "            end_time = msg.split(\"end:\")[1]\n",
    "            time_str = \"user defined\"\n",
    "          # end if/elifs\n",
    "\n",
    "          fname = self.data_dir + \"/leaderboard_\" + method + \"_start\" + \\\n",
    "            start_time + \"_\" + end_time + \".txt\"\n",
    "          if \"rtt lbAll\".lower() in msg:\n",
    "            fname = fname.replace(\"/leaderboard_\", \"/leaderboardSharded_\")\n",
    "          # end if\n",
    "\n",
    "          if os.path.exists(fname) and os.stat(fname).st_size != 0:\n",
    "            msg2  = \"Okay, grabbing the updated \" + method + \" leaderboard for \"\n",
    "            msg2 += time_str + \" data range. Note it'll take me ~30-60s to update (y'all raid hard :O\\n\\n\"\n",
    "            await channel.send(msg2)\n",
    "\n",
    "            msg2 = \"In the meantime, here's the last version of that leaderboard.\\n\"\n",
    "            with open(fname, \"r\") as fid:\n",
    "              for line in fid:\n",
    "                msg2 += line\n",
    "              # end for\n",
    "            # end with\n",
    "            dmsg = await channel.send(msg2)\n",
    "            edit = True\n",
    "          else:\n",
    "            msg2  = \"Okay, grabbing the \" + method + \" leaderboard for \"\n",
    "            msg2 += time_str + \" data range for the first time.\"\n",
    "            msg2 += \"\\nY'all raid hard so this can take a while (30s-60s)\"\n",
    "            msg2 += \"\\nso please be patient with me :D\"\n",
    "            await channel.send(msg2)\n",
    "          # end if/else\n",
    "\n",
    "          print(\"msg: \", msg)\n",
    "          if \"rtt lbAll\".lower() in msg:\n",
    "            print(\"in lbAll\")\n",
    "            #input(\">>\")\n",
    "            msg2 = await self.fetch_user_leaderboard(start_time=start_time,\n",
    "                   end_time=end_time, method=method, sharded=False)\n",
    "          else:\n",
    "            print(\"in reg lb\")\n",
    "            #input(\">>\")\n",
    "            msg2 = await self.fetch_user_leaderboard(start_time=start_time, \n",
    "                   end_time=end_time, method=method, sharded=True)\n",
    "          msg2 += \"\\nI've now updated the above leaderboard :)\"\n",
    "          print(\"discord bot hi here's the \" + method + \" leaderboard\")\n",
    "          print(msg2)\n",
    "\n",
    "        elif \"key\" in msg:\n",
    "          msg2 = \"Hi! These are the keywords I use to scrape for tweets:\\n\"\n",
    "          for keyword in self.keyword_query.split(\"OR\"):\n",
    "            msg2 += keyword + \"\\n\"\n",
    "          # end for\n",
    "\n",
    "        else:\n",
    "          msg2  = \"sorry! I didn't understand that command. Try 'rtt help(?)'\\n\"\n",
    "          msg2 += \"without the quotes...\"\n",
    "        # end if/elif/else\n",
    "        if not edit:\n",
    "          await channel.send(msg2)\n",
    "        else:\n",
    "          await dmsg.edit(content=msg2)\n",
    "      # end if rtt\n",
    "    # end async\n",
    "\n",
    "    secret = os.environ.get(\"rttBotPass\")\n",
    "    client.run(secret)\n",
    "  # end discord_bot\n",
    "# end class ScrapeTweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38cf4b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded linked_usernames!\n",
      "begin init_tweet\n",
      "success init_tweet\n",
      "tw_id:  1499858580568109058\n",
      "cr_un:  RooTroopNFT\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ScrapeTweets' object has no attribute 'auth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pr/mw9ljf9s05z6rjnmbffllg6w0000gn/T/ipykernel_32494/1130724624.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mtweet_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://twitter.com/RooTroopNFT/status/1499858580568109058?s=20&t=akwNHnsGLbvpDC-yfjvBVw\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0musername\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"thelunalabs\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mtweet_scrape_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverify_processed_tweet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musername\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"execution rate: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/pr/mw9ljf9s05z6rjnmbffllg6w0000gn/T/ipykernel_32494/3443392007.py\u001b[0m in \u001b[0;36mverify_processed_tweet\u001b[0;34m(self, tweet_url, username)\u001b[0m\n\u001b[1;32m   1457\u001b[0m         \u001b[0;34m+\u001b[0m \u001b[0;34m\"&tweet.fields=created_at\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1459\u001b[0;31m     os.system(self.curl_base + url + self.curl_header + self.auth + \n\u001b[0m\u001b[1;32m   1460\u001b[0m               \"' > \" + \"delete_me.txt\")\n\u001b[1;32m   1461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ScrapeTweets' object has no attribute 'auth'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  tweet_scrape_instance = ScrapeTweets()\n",
    "\n",
    "  tweet_url = \"https://twitter.com/RooTroopNFT/status/1499858580568109058?s=20&t=akwNHnsGLbvpDC-yfjvBVw\"\n",
    "  username = \"thelunalabs\"\n",
    "  tweet_scrape_instance.verify_processed_tweet(tweet_url, username)\n",
    "\n",
    "print(\"execution rate: \", time.time() - start)\n",
    "print(\"SUCCESS ScrapeTweets\")\n",
    "## end ScrapeTweets.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea83f762",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
